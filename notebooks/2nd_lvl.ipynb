{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/pulmonary_embolism/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from utils.logger import *\n",
    "from data.dataset import PEDatasetFt\n",
    "\n",
    "from model_zoo.models_lvl2 import *\n",
    "\n",
    "from utils.metric import rsna_metric\n",
    "from training.losses import RSNAWLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_arr(l):\n",
    "    l = re.sub('\\n', ' ', l[1:-1])\n",
    "    l = re.sub('\\.', '', l)\n",
    "    l = re.sub('\\s+', ' ', l).strip()\n",
    "    return np.array(l.split(' ')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../output/df_patient_level.csv')\n",
    "    df[IMG_TARGET] = df[IMG_TARGET].apply(str_to_arr)\n",
    "except:\n",
    "    df = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "    df = df.groupby(['StudyInstanceUID', 'SeriesInstanceUID'])[['SOPInstanceUID'] + EXAM_TARGETS + [IMG_TARGET]].agg(list).reset_index()\n",
    "    \n",
    "    ordered_targets = []\n",
    "    for study, series, names, tgt in tqdm(\n",
    "        df[['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'pe_present_on_image']].values\n",
    "    ):\n",
    "        imgs = sorted(os.listdir(IMG_PATH + f'{study}/{series}/'))\n",
    "        ordered_names = [n.split('_')[1][:-4] for n in imgs]\n",
    "        ordered_target = np.zeros(len(ordered_names))\n",
    "\n",
    "        for name, t in zip(names, tgt):\n",
    "            ordered_target[ordered_names.index(name)] = t\n",
    "\n",
    "        ordered_targets.append(ordered_target)\n",
    "    df[IMG_TARGET] = ordered_targets\n",
    "    \n",
    "    for c in EXAM_TARGETS:\n",
    "        df[c] = df[c].apply(lambda x: x[0])\n",
    "        \n",
    "    df['path'] = 'features_' + df['StudyInstanceUID'] + '_' + df['SeriesInstanceUID'] + '.npy'\n",
    "    df.to_csv('../output/df_patient_level.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path_preds'] = 'preds_' + df['StudyInstanceUID'] + '_' + df['SeriesInstanceUID'] + '.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PEDatasetFt(df, [FEATURES_PATH + \"resnext2/\", FEATURES_PATH + \"b3/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# for p in tqdm(df['path_preds']):\n",
    "#     pred = np.load(FEATURES_PATH + \"b3/\" + p)\n",
    "#     preds.append(pred)\n",
    "# preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exams_oof = np.load('../logs2/2020-10-25/4/pred_exams_oof.npy')\n",
    "pred_imgs_oof = np.load('../logs2/2020-10-25/4/pred_imgs_oof.npy')\n",
    "sizes_oof = np.load('../logs2/2020-10-25/4/sizes_oof.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exams_oof = np.ones(pred_exams_oof.shape) * 0.5\n",
    "pred_imgs_oof = np.ones(pred_imgs_oof.shape) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599361"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsna_metric(\n",
    "    dataset.img_targets,\n",
    "    dataset.exam_targets,\n",
    "    pred_imgs_oof,\n",
    "    pred_exams_oof,\n",
    "    sizes_oof,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(3584, use_msd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft, y_exam, y_img, size = dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_exam, logits_img = model(ft.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]),\n",
       " torch.Size([1, 1083]),\n",
       " torch.Size([1083, 3584]),\n",
       " torch.Size([9]),\n",
       " torch.Size([1083]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_exam.shape, logits_img.shape, ft.shape, y_exam.shape, y_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7245, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSNAWLoss(cuda=False)(y_img.unsqueeze(0), y_exam.unsqueeze(0), logits_img, logits_exam, size.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.724530650795709"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsna_metric(\n",
    "    y_img.unsqueeze(0).numpy(), \n",
    "    y_exam.unsqueeze(0).numpy(), \n",
    "    torch.sigmoid(logits_img).detach().numpy(), \n",
    "    torch.sigmoid(logits_exam).detach().numpy(), \n",
    "    size.unsqueeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train import *\n",
    "from utils.torch_utils import save_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchcontrib.optim import SWA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from training.sampler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    optimizer_name='adam',\n",
    "    loss_name='bce',\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    val_bs=32,\n",
    "    warmup_prop=0.1,\n",
    "    lr=1e-3,\n",
    "    swa_first_epoch=10,\n",
    "    verbose=1,\n",
    "):\n",
    "\n",
    "    optimizer = define_optimizer(optimizer_name, model.parameters(), lr=lr)\n",
    "    if swa_first_epoch < epochs:\n",
    "        optimizer = SWA(optimizer)\n",
    "\n",
    "#     loss_fct = nn.BCEWithLogitsLoss()\n",
    "    loss_fct = RSNAWLoss()\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=val_bs, \n",
    "        shuffle=False, \n",
    "        num_workers=8, \n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n",
    "    num_training_steps = int(epochs * len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps, num_training_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        start_time = time()\n",
    "\n",
    "        avg_loss = 0\n",
    "        t2 = time()\n",
    "        for x, y_exam, y_img, sizes in train_loader:\n",
    "            pred_exam, pred_img = model(x.cuda())\n",
    "            \n",
    "#             loss = loss_fct(pred_exam, y_exam.cuda()) + loss_fct(pred_img, y_img.cuda())\n",
    "            loss = loss_fct(y_img.cuda(), y_exam.cuda(), pred_img, pred_exam, sizes.cuda())\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "        if epoch + 1 >= swa_first_epoch:\n",
    "            optimizer.update_swa()\n",
    "            optimizer.swap_swa_sgd()\n",
    "\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        sizes = np.empty((0))\n",
    "        pred_exams = np.empty((0, NUM_EXAM_TARGETS))\n",
    "        pred_imgs = np.empty((0, val_dataset.max_len))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y_exam, y_img, size in val_loader:\n",
    "                pred_exam, pred_img = model(x.cuda())\n",
    "                \n",
    "#                 loss = loss_fct(pred_exam.detach(), y_exam.cuda()) + loss_fct(pred_img.detach(), y_img.cuda())\n",
    "                loss = loss_fct(y_img.cuda(), y_exam.cuda(), pred_img.detach(), pred_exam.detach(), size.cuda())\n",
    "                \n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "            \n",
    "                pred_exams = np.concatenate([pred_exams, torch.sigmoid(pred_exam).detach().cpu().numpy()])\n",
    "                pred_imgs = np.concatenate([pred_imgs, torch.sigmoid(pred_img).detach().cpu().numpy()])\n",
    "                sizes = np.concatenate([sizes, size.numpy()])\n",
    "                \n",
    "        score = rsna_metric(\n",
    "            val_dataset.img_targets,\n",
    "            val_dataset.exam_targets,\n",
    "            pred_imgs,\n",
    "            pred_exams,\n",
    "            sizes,\n",
    "        )\n",
    "\n",
    "        if epoch + 1 >= swa_first_epoch and epoch < epochs - 1:\n",
    "            optimizer.swap_swa_sgd()\n",
    "\n",
    "        elapsed_time = time() - start_time\n",
    "        if (epoch + 1) % verbose == 0:\n",
    "            elapsed_time = elapsed_time * verbose\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:02d}/{epochs:02d} \\t lr={lr:.1e} \\t t={elapsed_time:.0f}s  \\t loss={avg_loss:.3f} \\t \",\n",
    "                end=\"\",\n",
    "            )\n",
    "            print(\n",
    "                f\"val_loss={avg_val_loss:.3f}\\t score={score:.4f}\"\n",
    "            )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return pred_exams, pred_imgs, sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_utils import seed_everything, count_parameters\n",
    "\n",
    "\n",
    "def train(config, df_train, df_val, fold, log_folder=''):\n",
    "    \"\"\"\n",
    "    Trains and validate a model\n",
    "\n",
    "    Args:\n",
    "        config (Config): Parameters.\n",
    "        df_train (pandas dataframe): Training metadata.\n",
    "        df_val (pandas dataframe): Validation metadata.\n",
    "        fold (int): Selected fold.\n",
    "        log_folder (str, optional): Folder to logs results to. Defaults to ''.\n",
    "\n",
    "    Returns:\n",
    "        np array: Validation predictions.\n",
    "        pandas dataframe: Training history.\n",
    "    \"\"\"\n",
    "\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    model = RNNModel(\n",
    "        ft_dim=config.ft_dim, \n",
    "        lstm_dim=config.lstm_dim,\n",
    "        dense_dim=config.dense_dim,\n",
    "        logit_dim=config.logit_dim,\n",
    "        use_msd=config.use_msd,\n",
    "    ).cuda()\n",
    "        \n",
    "    model.zero_grad()\n",
    "\n",
    "    train_dataset = PEDatasetFt(df_train, max_len=config.max_len, paths=config.ft_path)\n",
    "    val_dataset = PEDatasetFt(df_val, max_len=config.max_len, paths=config.ft_path)\n",
    "        \n",
    "    n_parameters = count_parameters(model)\n",
    "    print(f\"    -> {len(train_dataset)} training images\")\n",
    "    print(f\"    -> {len(val_dataset)} validation images\")\n",
    "    print(f\"    -> {n_parameters} trainable parameters\\n\")\n",
    "\n",
    "    pred_exams, pred_imgs, sizes = fit(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        optimizer_name=config.optimizer,\n",
    "        loss_name=config.loss,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        val_bs=config.val_bs,\n",
    "        lr=config.lr,\n",
    "        warmup_prop=config.warmup_prop,\n",
    "        swa_first_epoch=config.swa_first_epoch,\n",
    "    )\n",
    "\n",
    "    if config.save_weights:\n",
    "        save_model_weights(\n",
    "            model,\n",
    "            f\"{config.name}_{fold}.pt\",\n",
    "            cp_folder=log_folder,\n",
    "        )\n",
    "        \n",
    "    return pred_exams, pred_imgs, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(config, df, log_folder=''):\n",
    "    \"\"\"\n",
    "    Performs a patient grouped k-fold cross validation.\n",
    "    The following things are saved to the log folder :\n",
    "    oof predictions, val predictions, val indices, histories\n",
    "\n",
    "    Args:\n",
    "        config (Config): Parameters.\n",
    "        df (pandas dataframe): Metadata.\n",
    "        log_folder (str, optional): Folder to logs results to. Defaults to ''.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    pred_exams_oof = np.zeros((len(df), NUM_EXAM_TARGETS))\n",
    "    pred_imgs_oof = np.zeros((len(df), config.max_len))\n",
    "    sizes_oof = np.zeros(len(df))\n",
    "        \n",
    "    kf = KFold(n_splits=config.k)\n",
    "    splits = list(kf.split(X=df))\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(splits):\n",
    "        if i in config.selected_folds:\n",
    "            print(f\"\\n-------------   Fold {i + 1} / {config.k}  -------------\\n\")\n",
    "\n",
    "            df_train = df.iloc[train_idx].copy()\n",
    "            df_val = df.iloc[val_idx].copy()\n",
    "\n",
    "            pred_exams, pred_imgs, sizes = train(config, df_train, df_val, i, log_folder=log_folder)\n",
    "            \n",
    "            pred_exams_oof[val_idx] = pred_exams\n",
    "            pred_imgs_oof[val_idx] = pred_imgs\n",
    "            sizes_oof[val_idx] = sizes\n",
    "            \n",
    "#             break\n",
    "    \n",
    "    return pred_exams_oof, pred_imgs_oof, sizes_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "    save_weights = True\n",
    "    max_len = 400\n",
    "    \n",
    "    ft_path = [\n",
    "        FEATURES_PATH + \"b3/\", \n",
    "#         FEATURES_PATH + \"resnext2/\", \n",
    "    ]\n",
    "\n",
    "    # k-fold\n",
    "    k = 5\n",
    "    selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # Model\n",
    "    ft_dim = 1536\n",
    "    lstm_dim = 256\n",
    "    dense_dim = 256\n",
    "    logit_dim = 256\n",
    "    use_msd = True\n",
    "    \n",
    "    # Training\n",
    "    loss = \"BCEWithLogitsLoss\"\n",
    "    optimizer = \"Adam\"\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    swa_first_epoch = 7\n",
    "    lr = 5e-3\n",
    "    warmup_prop = 0.\n",
    "    val_bs = 32\n",
    "\n",
    "    name = \"rnn_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging results to ../logs2/2020-10-26/5/\n"
     ]
    }
   ],
   "source": [
    "log_folder = prepare_log_folder(LOG_PATH_2)\n",
    "print(f'Logging results to {log_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_df = save_config(Config, log_folder + 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "pred_exams_oof, pred_imgs_oof, sizes_oof = k_fold(Config, df, log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(log_folder + 'pred_exams_oof.npy', pred_exams_oof)\n",
    "np.save(log_folder + 'pred_imgs_oof.npy', pred_imgs_oof)\n",
    "np.save(log_folder + 'sizes_oof.npy', sizes_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsna_metric(\n",
    "    dataset.img_targets,\n",
    "    dataset.exam_targets,\n",
    "    pred_imgs_oof,\n",
    "    pred_exams_oof,\n",
    "    sizes_oof,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
